# Emotion-Lite AI: A Minimalist Framework for Emotion-Driven Agents
## TL;DR
This repo proposes a minimalist emotional framework for artificial agents:  
**Desire** drives behavior, **anxiety** regulates it.  
Forget anger, frustration, or sadnessâ€”they're emergent, not essential.

## ğŸš§ Concept Overview

This repo presents a conceptual model for designing AI agents driven by **only two emotional primitives**:

- **Desire** â€“ the pursuit of goals or rewards
- **Anxiety** â€“ the tension or urgency caused by obstacles, uncertainty, or the risk of failure

The thesis is simple:

> Human behaviorâ€”and by extension, AI agent behaviorâ€”can be effectively modeled by desire constrained by anxiety. Anger, frustration, sadness, and other complex emotions may be unnecessary for functional intelligence or adaptive behavior.

---

## ğŸ’¡ Core Idea

### Traditional Emotional AI:
- Attempts to simulate a wide range of human emotions (joy, anger, sadness, empathy)
- Is complex, hard to control, and often unnecessary outside of high-social-fidelity use cases

### Emotion-Lite AI:
- Defines an agentâ€™s **emotional state as a scalar or bounded range**, constrained to a tension between goal pursuit and obstacle awareness
- Uses **anxiety** as a dynamic internal signal to trigger planning shifts, caution, retries, or abandonment
- Maintains **desire** as a driving force behind decision-making, exploration, and goal-directed behavior

---

## ğŸ§  Design Outline

```python
class EmotionLiteAgent:
    def __init__(self):
        self.desires = []          # List of goals
        self.anxiety = 0.0         # Scalar, 0.0 = calm, 1.0 = panic
        self.emotion_state = "engaged"  # Or "cautious", "hesitant", etc.

    def evaluate_environment(self, stimuli):
        if stimuli.obstacle:
            self.anxiety += 0.1
        if stimuli.progress:
            self.anxiety = max(0.0, self.anxiety - 0.05)

    def plan_behavior(self):
        if self.anxiety < 0.3:
            return plan_direct_action(self.desires)
        elif self.anxiety < 0.7:
            return plan_with_fallbacks(self.desires)
        else:
            return retreat_or_request_help()

```
## ğŸ” Why Not More Emotions?
> You donâ€™t need angerâ€”just an anxious desire that isnâ€™t being fulfilled.
> You donâ€™t need sadnessâ€”just lowered desire intensity over time.
> You donâ€™t need frustrationâ€”just escalating anxiety under blocked conditions.
## This framework:

Simplifies affective modeling for agents
- Enables more predictable and stable behavior
- Avoids manipulation or emotional uncanny valley
- Encourages action-oriented, interpretable behavior

## ğŸŒ Use Cases
- Autonomous agents and copilots with bounded emotional variance
- Industrial robotics or field systems needing goal adaptation under uncertainty
- AI safety agents requiring emotional transparency and predictability
- Simulation AI for games, tutoring systems, or virtual companions

## ğŸ“Œ Future Directions
- Expand with bounded memory (emotions influenced by past events)
- Combine with reinforcement learning to tune anxiety as a pseudo-intrinsic reward signal
- Investigate how this maps to real-world human-expert systems (e.g., pilots, surgeons, military)

## âœï¸ Author
Im just a web dev :)

## âš¡ Quote Worth Remembering
â€œEmotion isnâ€™t decorationâ€”itâ€™s regulation. And all an AI really needs is a desire to succeed, and just enough anxiety to avoid failing.â€

## ğŸ“¥ Feedback
Rs and discussions welcome. This is not a codebaseâ€”yetâ€”but a prompt for systems designers, AI safety thinkers, and anyone who believes simple things done well can reshape how we think about intelligence.
